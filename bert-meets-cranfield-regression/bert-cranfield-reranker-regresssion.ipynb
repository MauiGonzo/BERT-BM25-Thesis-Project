{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39nDDLhfg9CT"
   },
   "source": [
    "# BERT Meets Cranfield - Regression Approach\n",
    "instead of classifing the lot we can also do regression. The trick is to assign a good conversion table in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJMgZk2Qhhwt"
   },
   "source": [
    "## Environement Setup\n",
    "Connect to Drive by clicking on the icon, this only works when this colab is generated via the Drive menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1633108642179,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "qhmlD-V7hTk7",
    "outputId": "b39bc401-36b2-46b3-ae32-61bde20c8a1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/BERT-BM25-Thesis-Project/bert-meets-cranfield-regression/Code\n"
     ]
    }
   ],
   "source": [
    "# %cd /content/drive/MyDrive/COMPUTING SCIENCE/THESIS_PROJECT/bert-meets-cranfield-regression/Code\n",
    "%cd /home/jupyter/BERT-BM25-Thesis-Project/bert-meets-cranfield-regression/Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3679,
     "status": "ok",
     "timestamp": 1633108646067,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "ug0uTkZbhqui",
    "outputId": "7f5b3b46-1478-4a46-c852-4c82ba7e95d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.19.1 in /opt/conda/lib/python3.7/site-packages (from -r ../requirements.txt (line 1)) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.5.2 in /opt/conda/lib/python3.7/site-packages (from -r ../requirements.txt (line 2)) (1.7.1)\n",
      "Requirement already satisfied: rank_bm25>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from -r ../requirements.txt (line 3)) (0.2.1)\n",
      "Requirement already satisfied: transformers>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from -r ../requirements.txt (line 4)) (4.10.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from -r ../requirements.txt (line 5)) (1.9.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (2.25.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (2021.8.28)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (0.10.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (4.8.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (4.62.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.12 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (0.0.17)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (0.0.45)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->-r ../requirements.txt (line 5)) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers>=3.1.0->-r ../requirements.txt (line 4)) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers>=3.1.0->-r ../requirements.txt (line 4)) (3.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.1.0->-r ../requirements.txt (line 4)) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.1.0->-r ../requirements.txt (line 4)) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.1.0->-r ../requirements.txt (line 4)) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.1.0->-r ../requirements.txt (line 4)) (2021.5.30)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=3.1.0->-r ../requirements.txt (line 4)) (8.0.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=3.1.0->-r ../requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=3.1.0->-r ../requirements.txt (line 4)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3096,
     "status": "ok",
     "timestamp": 1633108649149,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "pDhoaP1EiFVk"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import data_utils\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import importlib\n",
    "import timeit\n",
    "# from transformers import BertForSequenceClassification, BertTokenizer, BertForMaskedLM, BertForNextSentencePrediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1633108649149,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "oZUZPtxV77yM",
    "outputId": "b2413365-3c95-4d19-ec90-944b1dd2285f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_utils' from '/home/jupyter/BERT-BM25-Thesis-Project/bert-meets-cranfield-regression/Code/data_utils.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call after making any changes in utils.py\n",
    "importlib.reload(utils) \n",
    "importlib.reload(data_utils) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6g7v8s2OdSu"
   },
   "source": [
    "## Notes\n",
    "The following changes apply:\n",
    " * labels have been replaced with the regression_labels\n",
    " * the prepare model routine now works with a `num_labels=1` setting the taks head in regression mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 440,
     "status": "ok",
     "timestamp": 1633108649564,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "20Ly0htminFe"
   },
   "outputs": [],
   "source": [
    "# import utils\n",
    "# import data_utils\n",
    "# from operator import itemgetter\n",
    "# import os\n",
    "\n",
    "# ========================================\n",
    "#               Hyper-Parameters\n",
    "# ========================================\n",
    "SEED = 76\n",
    "MODE = 'Re-ranker'\n",
    "MODEL_TYPE = 'bert-base-uncased'\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "TOP_BM25 = 100\n",
    "MAP_CUT = 100\n",
    "NDCG_CUT = 20\n",
    "if MODE == 'Full-ranker':\n",
    "    TEST_BATCH_SIZE = 1400\n",
    "else:\n",
    "    TEST_BATCH_SIZE = 100\n",
    "\n",
    "CONVERSION_TABLE = [[0, 1, 2, 3, 4],[0, 1, 1, 1, 1]] \n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "utils.initialize_random_generators(SEED)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "def train_test():\n",
    "    print(\"# ========================================\")\n",
    "    print(\"#               Hyper-Parameters\")\n",
    "    print(MODE)\n",
    "    print(MODEL_TYPE)\n",
    "    print(LEARNING_RATE)\n",
    "    print(MAX_LENGTH)\n",
    "    print(BATCH_SIZE)\n",
    "    print(EPOCHS)\n",
    "    print(\"# ========================================\")\n",
    "    print(\"#               Experiment-Settings\")\n",
    "    print(\"CONVERSION TABLE:\")\n",
    "    print(CONVERSION_TABLE )\n",
    "    print(\"# ========================================\")\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    device = utils.get_gpu_device()\n",
    "    if not os.path.exists('../Output_Folder'):\n",
    "        os.makedirs('../Output_Folder')\n",
    "\n",
    "    queries = data_utils.get_queries('../Data/cran/cran.qry')\n",
    "    corpus = data_utils.get_corpus('../Data/cran/cran.all.1400')\n",
    "    rel_fed = data_utils.get_judgments('../Data/cran/cranqrel')\n",
    "\n",
    "    # labels = utils.get_binary_labels(rel_fed)\n",
    "\n",
    "    # get labels for the regression input\n",
    "    regression_labels = utils.get_regression_labels(rel_fed, CONVERSION_TABLE)\n",
    "\n",
    "    tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "    tokenized_queries = [query.split(\" \") for query in queries]\n",
    "\n",
    "    bm25, bm25_top_n = utils.get_bm25_top_results(tokenized_corpus, tokenized_queries, TOP_BM25)\n",
    "\n",
    "    padded_all, attention_mask_all, token_type_ids_all, temp_feedback = utils.bert_tokenizer(MODE, bm25_top_n, corpus,\n",
    "                                                                                            #  labels, queries,\n",
    "                                                                                             regression_labels, queries,\n",
    "                                                                                             MAX_LENGTH, MODEL_TYPE)\n",
    "\n",
    "    # ========================================\n",
    "    #               Folds\n",
    "    # ========================================\n",
    "    mrr_bm25_list, map_bm25_list, ndcg_bm25_list = [], [], []\n",
    "    mrr_bert_list, map_bert_list, ndcg_bert_list = [], [], []\n",
    "    mrr_bm25, map_bm25, ndcg_bm25 = 0, 0, 0\n",
    "    mrr_bert, map_bert, ndcg_bert = 0, 0, 0\n",
    "\n",
    "    for fold_number in range(1, 6):\n",
    "        print('======== Fold {:} / {:} ========'.format(fold_number, 5))\n",
    "        train_index, test_index = data_utils.load_fold(fold_number)\n",
    "\n",
    "        padded, attention_mask, token_type_ids = [], [], []\n",
    "        if MODE == 'Re-ranker':\n",
    "            padded, attention_mask, token_type_ids = padded_all, attention_mask_all, token_type_ids_all\n",
    "        else:\n",
    "            temp_feedback = []\n",
    "            for query_num in range(0, len(bm25_top_n)):\n",
    "                if query_num in test_index:\n",
    "                    doc_nums = range(0, 1400)\n",
    "                else:\n",
    "                    doc_nums = bm25_top_n[query_num]\n",
    "                padded.append(list(itemgetter(*doc_nums)(padded_all[query_num])))\n",
    "                attention_mask.append(list(itemgetter(*doc_nums)(attention_mask_all[query_num])))\n",
    "                token_type_ids.append(list(itemgetter(*doc_nums)(token_type_ids_all[query_num])))\n",
    "                temp_feedback.append(list(itemgetter(*doc_nums)(regression_labels[query_num])))\n",
    "                # temp_feedback.append(list(itemgetter(*doc_nums)(labels[query_num])))\n",
    "\n",
    "        train_dataset = data_utils.get_tensor_dataset(train_index, padded, attention_mask, token_type_ids,\n",
    "                                                      temp_feedback)\n",
    "        test_dataset = data_utils.get_tensor_dataset(test_index, padded, attention_mask, token_type_ids, temp_feedback)\n",
    "\n",
    "        mrr_bm25, map_bm25, ndcg_bm25, mrr_bm25_list, map_bm25_list, ndcg_bm25_list = utils.get_bm25_results(\n",
    "            mrr_bm25_list, map_bm25_list, ndcg_bm25_list, test_index, tokenized_queries, bm25, mrr_bm25, map_bm25,\n",
    "            ndcg_bm25, rel_fed, fold_number, MAP_CUT, NDCG_CUT)\n",
    "\n",
    "        train_dataloader, test_dataloader, model, optimizer, scheduler = utils.model_preparation(MODEL_TYPE, train_dataset,\n",
    "                                                                                                 test_dataset,\n",
    "                                                                                                 BATCH_SIZE, TEST_BATCH_SIZE,\n",
    "                                                                                                 LEARNING_RATE, EPOCHS)\n",
    "        # ========================================\n",
    "        #               Training Loop\n",
    "        # ========================================\n",
    "        epochs_train_loss, epochs_val_loss = [], []\n",
    "        for epoch_i in range(0, EPOCHS):\n",
    "            # ========================================\n",
    "            #               Training\n",
    "            # ========================================\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n",
    "            print('Training...')\n",
    "            model, optimizer, scheduler = utils.training(model, train_dataloader, device, optimizer, scheduler)\n",
    "        # ========================================\n",
    "        #               Testing\n",
    "        # ========================================\n",
    "        print('Testing...')\n",
    "        mrr_bert, map_bert, ndcg_bert, mrr_bert_list, map_bert_list, ndcg_bert_list = utils.testing(MODE, model,\n",
    "                                                                                                    test_dataloader,\n",
    "                                                                                                    device, test_index,\n",
    "                                                                                                    bm25_top_n,\n",
    "                                                                                                    mrr_bert_list,\n",
    "                                                                                                    map_bert_list,\n",
    "                                                                                                    ndcg_bert_list,\n",
    "                                                                                                    mrr_bert, map_bert,\n",
    "                                                                                                    ndcg_bert, rel_fed,\n",
    "                                                                                                    fold_number,\n",
    "                                                                                                    MAP_CUT, NDCG_CUT)\n",
    "    print(\"  BM25 MRR:  \" + \"{:.4f}\".format(mrr_bm25 / 5))\n",
    "    print(\"  BM25 MAP:  \" + \"{:.4f}\".format(map_bm25 / 5))\n",
    "    print(\"  BM25 NDCG: \" + \"{:.4f}\".format(ndcg_bm25 / 5))\n",
    "\n",
    "    print(\"  BERT MRR:  \" + \"{:.4f}\".format(mrr_bert / 5))\n",
    "    print(\"  BERT MAP:  \" + \"{:.4f}\".format(map_bert / 5))\n",
    "    print(\"  BERT NDCG: \" + \"{:.4f}\".format(ndcg_bert / 5))\n",
    "\n",
    "    utils.t_test(mrr_bm25_list, mrr_bert_list, 'MRR')\n",
    "    utils.t_test(map_bm25_list, map_bert_list, 'MAP')\n",
    "    utils.t_test(ndcg_bm25_list, ndcg_bert_list, 'NDCG')\n",
    "   \n",
    "    stop = timeit.default_timer()\n",
    "    wall_time = (stop - start) / 60 \n",
    "\n",
    "    print('Time: ', wall_time, ' min')\n",
    "\n",
    "    # utils.results_to_csv('./mrr_bm25_list.csv', mrr_bm25_list)\n",
    "    # utils.results_to_csv('./mrr_bert_list.csv', mrr_bert_list)\n",
    "    # utils.results_to_csv('./map_bm25_list.csv', map_bm25_list)\n",
    "    # utils.results_to_csv('./map_bert_list.csv', map_bert_list)\n",
    "    # utils.results_to_csv('./ndcg_bm25_list.csv', ndcg_bm25_list)\n",
    "    # utils.results_to_csv('./ndcg_bert_list.csv', ndcg_bert_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpM8JkXO5atm"
   },
   "source": [
    "# Testing part\n",
    "note that the `lr=2e-05` and `EPOCHS=1` already have been done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 920,
     "referenced_widgets": [
      "b0709620b1b449c995b4f0e565356a51",
      "8d2804829b444947a1f1186c8898d6d2",
      "e70ff01f690542d6a36b5b995b6a8d5a",
      "3ebab73c8f9a487fb5db5584b914540a",
      "13cede8e4e26443e99deb0599e16cad7",
      "4193ea95c41740b3a9df08564e8619e2",
      "6d2931bd2c3a48ffb08d62185b54a05b",
      "daecc23330a34a65bf1479984b20f583",
      "4c3c7b15c4db4b55b5b2377bd513d327",
      "3a2c1607b49741c3a9e027accb8125b7",
      "5dc9687148f94e91992a3ddb1659dd25"
     ]
    },
    "executionInfo": {
     "elapsed": 668116,
     "status": "error",
     "timestamp": 1633109317669,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "H8999UNn1iqs",
    "outputId": "cb6a6358-ccee-42fe-faf8-30b3c58163e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================\n",
      "#               Hyper-Parameters\n",
      "Re-ranker\n",
      "bert-base-uncased\n",
      "2e-05\n",
      "128\n",
      "32\n",
      "2\n",
      "# ========================================\n",
      "#               Experiment-Settings\n",
      "CONVERSION TABLE:\n",
      "[[0, 1, 2, 3, 4], [0, 1, 1, 1, 1]]\n",
      "# ========================================\n",
      "GPU Type: Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2718/2365499731.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2718/3397896296.py\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m                                                                                             \u001b[0;31m#  labels, queries,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                                                                                              \u001b[0mregression_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                                                                                              MAX_LENGTH, MODEL_TYPE)\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# ========================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BERT-BM25-Thesis-Project/bert-meets-cranfield-regression/Code/utils.py\u001b[0m in \u001b[0;36mbert_tokenizer\u001b[0;34m(mode, bm25_top_n, corpus, labels, queries, max_length, model_type)\u001b[0m\n\u001b[1;32m    139\u001b[0m                                         \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                                         \u001b[0mpad_to_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                                         return_tensors='pt')\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0mcurrent_padded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mcurrent_attention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m             )\n\u001b[1;32m   2422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2489\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2490\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2491\u001b[0m         )\n\u001b[1;32m   2492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         return self.prepare_for_model(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mno_split_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_split_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[0;34m(tok_list, text)\u001b[0m\n\u001b[1;32m    354\u001b[0m                     (\n\u001b[1;32m    355\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                     )\n\u001b[1;32m    358\u001b[0m                 )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    354\u001b[0m                     (\n\u001b[1;32m    355\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                     )\n\u001b[1;32m    358\u001b[0m                 )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# If the token is part of the never_split set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip_accents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_strip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_split_on_punc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhitespace_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_run_split_on_punc\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0m_is_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mstart_new_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_is_punctuation\u001b[0;34m(char)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Punctuation class but we treat them as punctuation anyways, for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# consistency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m33\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m47\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m58\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m91\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m123\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m126\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 2 \n",
    "train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1633109317415,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "wF9fbKAn2BcA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================\n",
      "#               Hyper-Parameters\n",
      "Re-ranker\n",
      "bert-base-uncased\n",
      "3e-05\n",
      "128\n",
      "32\n",
      "1\n",
      "# ========================================\n",
      "#               Experiment-Settings\n",
      "CONVERSION TABLE:\n",
      "[[0, 1, 2, 3, 4], [0, 1, 1, 1, 1]]\n",
      "# ========================================\n",
      "GPU Type: Tesla T4\n",
      "======== Fold 1 / 5 ========\n",
      "MRR:  0.7837\n",
      "MAP:  0.3493\n",
      "NDCG: 0.5011\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0426\n",
      "Testing...\n",
      "  Test MRR:  0.8180\n",
      "  Test MAP:  0.4082\n",
      "  Test NDCG: 0.5464\n",
      "45\n",
      "======== Fold 2 / 5 ========\n",
      "MRR:  0.6596\n",
      "MAP:  0.3036\n",
      "NDCG: 0.4546\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0464\n",
      "Testing...\n",
      "  Test MRR:  0.7133\n",
      "  Test MAP:  0.3477\n",
      "  Test NDCG: 0.4975\n",
      "90\n",
      "======== Fold 3 / 5 ========\n",
      "MRR:  0.7611\n",
      "MAP:  0.3341\n",
      "NDCG: 0.4826\n",
      "135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0421\n",
      "Testing...\n",
      "  Test MRR:  0.8616\n",
      "  Test MAP:  0.4389\n",
      "  Test NDCG: 0.5707\n",
      "135\n",
      "======== Fold 4 / 5 ========\n",
      "MRR:  0.6859\n",
      "MAP:  0.3317\n",
      "NDCG: 0.4408\n",
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0434\n",
      "Testing...\n",
      "  Test MRR:  0.7447\n",
      "  Test MAP:  0.3972\n",
      "  Test NDCG: 0.5004\n",
      "180\n",
      "======== Fold 5 / 5 ========\n",
      "MRR:  0.7796\n",
      "MAP:  0.3182\n",
      "NDCG: 0.4780\n",
      "225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0521\n",
      "Testing...\n",
      "  Test MRR:  0.8139\n",
      "  Test MAP:  0.3880\n",
      "  Test NDCG: 0.5381\n",
      "225\n",
      "  BM25 MRR:  0.7340\n",
      "  BM25 MAP:  0.3274\n",
      "  BM25 NDCG: 0.4714\n",
      "  BERT MRR:  0.7903\n",
      "  BERT MAP:  0.3960\n",
      "  BERT NDCG: 0.5306\n",
      "p-value MRR: 0.1024\n",
      "p-value MAP: 0.0050\n",
      "p-value NDCG: 0.0191\n",
      "Time:  39.045717645983316  min\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 3e-5\n",
    "EPOCHS = 1\n",
    "train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1633109317416,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "uLPqbYBd2GW9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================\n",
      "#               Hyper-Parameters\n",
      "Re-ranker\n",
      "bert-base-uncased\n",
      "3e-05\n",
      "128\n",
      "32\n",
      "2\n",
      "# ========================================\n",
      "#               Experiment-Settings\n",
      "CONVERSION TABLE:\n",
      "[[0, 1, 2, 3, 4], [0, 1, 1, 1, 1]]\n",
      "# ========================================\n",
      "GPU Type: Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Fold 1 / 5 ========\n",
      "MRR:  0.7837\n",
      "MAP:  0.3493\n",
      "NDCG: 0.5011\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0469\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0305\n",
      "Testing...\n",
      "  Test MRR:  0.8556\n",
      "  Test MAP:  0.4313\n",
      "  Test NDCG: 0.5792\n",
      "45\n",
      "======== Fold 2 / 5 ========\n",
      "MRR:  0.6596\n",
      "MAP:  0.3036\n",
      "NDCG: 0.4546\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0530\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0329\n",
      "Testing...\n",
      "  Test MRR:  0.7278\n",
      "  Test MAP:  0.3646\n",
      "  Test NDCG: 0.5131\n",
      "90\n",
      "======== Fold 3 / 5 ========\n",
      "MRR:  0.7611\n",
      "MAP:  0.3341\n",
      "NDCG: 0.4826\n",
      "135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0444\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0298\n",
      "Testing...\n",
      "  Test MRR:  0.8356\n",
      "  Test MAP:  0.4424\n",
      "  Test NDCG: 0.5812\n",
      "135\n",
      "======== Fold 4 / 5 ========\n",
      "MRR:  0.6859\n",
      "MAP:  0.3317\n",
      "NDCG: 0.4408\n",
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0451\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0307\n",
      "Testing...\n",
      "  Test MRR:  0.7500\n",
      "  Test MAP:  0.3954\n",
      "  Test NDCG: 0.4972\n",
      "180\n",
      "======== Fold 5 / 5 ========\n",
      "MRR:  0.7796\n",
      "MAP:  0.3182\n",
      "NDCG: 0.4780\n",
      "225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0421\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0296\n",
      "Testing...\n",
      "  Test MRR:  0.8416\n",
      "  Test MAP:  0.4304\n",
      "  Test NDCG: 0.5768\n",
      "225\n",
      "  BM25 MRR:  0.7340\n",
      "  BM25 MAP:  0.3274\n",
      "  BM25 NDCG: 0.4714\n",
      "  BERT MRR:  0.8021\n",
      "  BERT MAP:  0.4128\n",
      "  BERT NDCG: 0.5495\n",
      "p-value MRR: 0.0473\n",
      "p-value MAP: 0.0006\n",
      "p-value NDCG: 0.0021\n",
      "Time:  73.21226674481667  min\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 3e-5\n",
    "EPOCHS = 2\n",
    "train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1633109317416,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "BSM6OOze2RJM"
   },
   "outputs": [],
   "source": [
    "CONVERSION_TABLE = [[0, 1, 2, 3, 4],[-0.5, 1, 0.8, 0.6, 0.3]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1633109317417,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "3cPURYu82O7l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================\n",
      "#               Hyper-Parameters\n",
      "Re-ranker\n",
      "bert-base-uncased\n",
      "2e-05\n",
      "128\n",
      "32\n",
      "2\n",
      "# ========================================\n",
      "#               Experiment-Settings\n",
      "CONVERSION TABLE:\n",
      "[[0, 1, 2, 3, 4], [-0.5, 1, 0.8, 0.6, 0.3]]\n",
      "# ========================================\n",
      "GPU Type: Tesla T4\n",
      "======== Fold 1 / 5 ========\n",
      "MRR:  0.7837\n",
      "MAP:  0.3493\n",
      "NDCG: 0.5011\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0244\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0169\n",
      "Testing...\n",
      "  Test MRR:  0.8654\n",
      "  Test MAP:  0.4227\n",
      "  Test NDCG: 0.5767\n",
      "45\n",
      "======== Fold 2 / 5 ========\n",
      "MRR:  0.6596\n",
      "MAP:  0.3036\n",
      "NDCG: 0.4546\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0241\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0165\n",
      "Testing...\n",
      "  Test MRR:  0.7182\n",
      "  Test MAP:  0.3660\n",
      "  Test NDCG: 0.5101\n",
      "90\n",
      "======== Fold 3 / 5 ========\n",
      "MRR:  0.7611\n",
      "MAP:  0.3341\n",
      "NDCG: 0.4826\n",
      "135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0255\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0164\n",
      "Testing...\n",
      "  Test MRR:  0.8740\n",
      "  Test MAP:  0.4556\n",
      "  Test NDCG: 0.5880\n",
      "135\n",
      "======== Fold 4 / 5 ========\n",
      "MRR:  0.6859\n",
      "MAP:  0.3317\n",
      "NDCG: 0.4408\n",
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0242\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0161\n",
      "Testing...\n",
      "  Test MRR:  0.7493\n",
      "  Test MAP:  0.3947\n",
      "  Test NDCG: 0.5053\n",
      "180\n",
      "======== Fold 5 / 5 ========\n",
      "MRR:  0.7796\n",
      "MAP:  0.3182\n",
      "NDCG: 0.4780\n",
      "225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0221\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0153\n",
      "Testing...\n",
      "  Test MRR:  0.8319\n",
      "  Test MAP:  0.4294\n",
      "  Test NDCG: 0.5835\n",
      "225\n",
      "  BM25 MRR:  0.7340\n",
      "  BM25 MAP:  0.3274\n",
      "  BM25 NDCG: 0.4714\n",
      "  BERT MRR:  0.8077\n",
      "  BERT MAP:  0.4137\n",
      "  BERT NDCG: 0.5527\n",
      "p-value MRR: 0.0327\n",
      "p-value MAP: 0.0005\n",
      "p-value NDCG: 0.0014\n",
      "Time:  72.69267198805001  min\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 2\n",
    "train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1633109317417,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "RePuv2Tn2PTH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================\n",
      "#               Hyper-Parameters\n",
      "Re-ranker\n",
      "bert-base-uncased\n",
      "3e-05\n",
      "128\n",
      "32\n",
      "1\n",
      "# ========================================\n",
      "#               Experiment-Settings\n",
      "CONVERSION TABLE:\n",
      "[[0, 1, 2, 3, 4], [-0.5, 1, 0.8, 0.6, 0.3]]\n",
      "# ========================================\n",
      "GPU Type: Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Fold 1 / 5 ========\n",
      "MRR:  0.7837\n",
      "MAP:  0.3493\n",
      "NDCG: 0.5011\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0268\n",
      "Testing...\n",
      "  Test MRR:  0.8356\n",
      "  Test MAP:  0.4171\n",
      "  Test NDCG: 0.5512\n",
      "45\n",
      "======== Fold 2 / 5 ========\n",
      "MRR:  0.6596\n",
      "MAP:  0.3036\n",
      "NDCG: 0.4546\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0294\n",
      "Testing...\n",
      "  Test MRR:  0.6822\n",
      "  Test MAP:  0.3391\n",
      "  Test NDCG: 0.4753\n",
      "90\n",
      "======== Fold 3 / 5 ========\n",
      "MRR:  0.7611\n",
      "MAP:  0.3341\n",
      "NDCG: 0.4826\n",
      "135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0243\n",
      "Testing...\n",
      "  Test MRR:  0.8565\n",
      "  Test MAP:  0.4465\n",
      "  Test NDCG: 0.5853\n",
      "135\n",
      "======== Fold 4 / 5 ========\n",
      "MRR:  0.6859\n",
      "MAP:  0.3317\n",
      "NDCG: 0.4408\n",
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0247\n",
      "Testing...\n",
      "  Test MRR:  0.7331\n",
      "  Test MAP:  0.3900\n",
      "  Test NDCG: 0.4976\n",
      "180\n",
      "======== Fold 5 / 5 ========\n",
      "MRR:  0.7796\n",
      "MAP:  0.3182\n",
      "NDCG: 0.4780\n",
      "225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0222\n",
      "Testing...\n",
      "  Test MRR:  0.8142\n",
      "  Test MAP:  0.4079\n",
      "  Test NDCG: 0.5694\n",
      "225\n",
      "  BM25 MRR:  0.7340\n",
      "  BM25 MAP:  0.3274\n",
      "  BM25 NDCG: 0.4714\n",
      "  BERT MRR:  0.7843\n",
      "  BERT MAP:  0.4001\n",
      "  BERT NDCG: 0.5358\n",
      "p-value MRR: 0.1481\n",
      "p-value MAP: 0.0031\n",
      "p-value NDCG: 0.0107\n",
      "Time:  39.038887666966666  min\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 3e-5\n",
    "EPOCHS = 1\n",
    "train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "aborted",
     "timestamp": 1633109317418,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "EGlkwnaU2PWT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================\n",
      "#               Hyper-Parameters\n",
      "Re-ranker\n",
      "bert-base-uncased\n",
      "3e-05\n",
      "128\n",
      "32\n",
      "2\n",
      "# ========================================\n",
      "#               Experiment-Settings\n",
      "CONVERSION TABLE:\n",
      "[[0, 1, 2, 3, 4], [-0.5, 1, 0.8, 0.6, 0.3]]\n",
      "# ========================================\n",
      "GPU Type: Tesla T4\n",
      "======== Fold 1 / 5 ========\n",
      "MRR:  0.7837\n",
      "MAP:  0.3493\n",
      "NDCG: 0.5011\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0275\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0150\n",
      "Testing...\n",
      "  Test MRR:  0.8552\n",
      "  Test MAP:  0.4213\n",
      "  Test NDCG: 0.5706\n",
      "45\n",
      "======== Fold 2 / 5 ========\n",
      "MRR:  0.6596\n",
      "MAP:  0.3036\n",
      "NDCG: 0.4546\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0246\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0160\n",
      "Testing...\n",
      "  Test MRR:  0.7090\n",
      "  Test MAP:  0.3536\n",
      "  Test NDCG: 0.5030\n",
      "90\n",
      "======== Fold 3 / 5 ========\n",
      "MRR:  0.7611\n",
      "MAP:  0.3341\n",
      "NDCG: 0.4826\n",
      "135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0251\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0148\n",
      "Testing...\n",
      "  Test MRR:  0.8237\n",
      "  Test MAP:  0.4400\n",
      "  Test NDCG: 0.5748\n",
      "135\n",
      "======== Fold 4 / 5 ========\n",
      "MRR:  0.6859\n",
      "MAP:  0.3317\n",
      "NDCG: 0.4408\n",
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0256\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0153\n",
      "Testing...\n",
      "  Test MRR:  0.7712\n",
      "  Test MAP:  0.3929\n",
      "  Test NDCG: 0.4975\n",
      "180\n",
      "======== Fold 5 / 5 ========\n",
      "MRR:  0.7796\n",
      "MAP:  0.3182\n",
      "NDCG: 0.4780\n",
      "225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0244\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0161\n",
      "Testing...\n",
      "  Test MRR:  0.8278\n",
      "  Test MAP:  0.4281\n",
      "  Test NDCG: 0.5853\n",
      "225\n",
      "  BM25 MRR:  0.7340\n",
      "  BM25 MAP:  0.3274\n",
      "  BM25 NDCG: 0.4714\n",
      "  BERT MRR:  0.7974\n",
      "  BERT MAP:  0.4072\n",
      "  BERT NDCG: 0.5462\n",
      "p-value MRR: 0.0656\n",
      "p-value MAP: 0.0012\n",
      "p-value NDCG: 0.0030\n",
      "Time:  72.77926726286668  min\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 3e-5\n",
    "EPOCHS = 2\n",
    "train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UtBwA9C032Kf"
   },
   "outputs": [],
   "source": [
    "CONVERSION_TABLE = [[0, 1, 2, 3, 4],[-1, 1, 0.7, 0.53, 0.4]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8R5_D3cE32Kf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================\n",
      "#               Hyper-Parameters\n",
      "Re-ranker\n",
      "bert-base-uncased\n",
      "2e-05\n",
      "128\n",
      "32\n",
      "2\n",
      "# ========================================\n",
      "#               Experiment-Settings\n",
      "CONVERSION TABLE:\n",
      "[[0, 1, 2, 3, 4], [-1, 1, 0.7, 0.53, 0.4]]\n",
      "# ========================================\n",
      "GPU Type: Tesla T4\n",
      "======== Fold 1 / 5 ========\n",
      "MRR:  0.7837\n",
      "MAP:  0.3493\n",
      "NDCG: 0.5011\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0214\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0145\n",
      "Testing...\n",
      "  Test MRR:  0.8569\n",
      "  Test MAP:  0.4239\n",
      "  Test NDCG: 0.5736\n",
      "45\n",
      "======== Fold 2 / 5 ========\n",
      "MRR:  0.6596\n",
      "MAP:  0.3036\n",
      "NDCG: 0.4546\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0214\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0138\n",
      "Testing...\n",
      "  Test MRR:  0.7030\n",
      "  Test MAP:  0.3570\n",
      "  Test NDCG: 0.5040\n",
      "90\n",
      "======== Fold 3 / 5 ========\n",
      "MRR:  0.7611\n",
      "MAP:  0.3341\n",
      "NDCG: 0.4826\n",
      "135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0234\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0151\n",
      "Testing...\n",
      "  Test MRR:  0.8875\n",
      "  Test MAP:  0.4480\n",
      "  Test NDCG: 0.5906\n",
      "135\n",
      "======== Fold 4 / 5 ========\n",
      "MRR:  0.6859\n",
      "MAP:  0.3317\n",
      "NDCG: 0.4408\n",
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0232\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0146\n",
      "Testing...\n",
      "  Test MRR:  0.7459\n",
      "  Test MAP:  0.4036\n",
      "  Test NDCG: 0.5005\n",
      "180\n",
      "======== Fold 5 / 5 ========\n",
      "MRR:  0.7796\n",
      "MAP:  0.3182\n",
      "NDCG: 0.4780\n",
      "225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0218\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0132\n",
      "Testing...\n",
      "  Test MRR:  0.7923\n",
      "  Test MAP:  0.4143\n",
      "  Test NDCG: 0.5717\n",
      "225\n",
      "  BM25 MRR:  0.7340\n",
      "  BM25 MAP:  0.3274\n",
      "  BM25 NDCG: 0.4714\n",
      "  BERT MRR:  0.7971\n",
      "  BERT MAP:  0.4094\n",
      "  BERT NDCG: 0.5481\n",
      "p-value MRR: 0.0684\n",
      "p-value MAP: 0.0009\n",
      "p-value NDCG: 0.0024\n",
      "Time:  72.76701461618335  min\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 2\n",
    "train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "y7mRRM6n32Kf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================\n",
      "#               Hyper-Parameters\n",
      "Re-ranker\n",
      "bert-base-uncased\n",
      "3e-05\n",
      "128\n",
      "32\n",
      "1\n",
      "# ========================================\n",
      "#               Experiment-Settings\n",
      "CONVERSION TABLE:\n",
      "[[0, 1, 2, 3, 4], [-1, 1, 0.7, 0.53, 0.4]]\n",
      "# ========================================\n",
      "GPU Type: Tesla T4\n",
      "======== Fold 1 / 5 ========\n",
      "MRR:  0.7837\n",
      "MAP:  0.3493\n",
      "NDCG: 0.5011\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0245\n",
      "Testing...\n",
      "  Test MRR:  0.8295\n",
      "  Test MAP:  0.4095\n",
      "  Test NDCG: 0.5492\n",
      "45\n",
      "======== Fold 2 / 5 ========\n",
      "MRR:  0.6596\n",
      "MAP:  0.3036\n",
      "NDCG: 0.4546\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0210\n",
      "Testing...\n",
      "  Test MRR:  0.7190\n",
      "  Test MAP:  0.3578\n",
      "  Test NDCG: 0.4972\n",
      "90\n",
      "======== Fold 3 / 5 ========\n",
      "MRR:  0.7611\n",
      "MAP:  0.3341\n",
      "NDCG: 0.4826\n",
      "135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0212\n",
      "Testing...\n",
      "  Test MRR:  0.8432\n",
      "  Test MAP:  0.4360\n",
      "  Test NDCG: 0.5728\n",
      "135\n",
      "======== Fold 4 / 5 ========\n",
      "MRR:  0.6859\n",
      "MAP:  0.3317\n",
      "NDCG: 0.4408\n",
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0212\n",
      "Testing...\n",
      "  Test MRR:  0.7538\n",
      "  Test MAP:  0.3908\n",
      "  Test NDCG: 0.4912\n",
      "180\n",
      "======== Fold 5 / 5 ========\n",
      "MRR:  0.7796\n",
      "MAP:  0.3182\n",
      "NDCG: 0.4780\n",
      "225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0213\n",
      "Testing...\n",
      "  Test MRR:  0.8354\n",
      "  Test MAP:  0.4135\n",
      "  Test NDCG: 0.5646\n",
      "225\n",
      "  BM25 MRR:  0.7340\n",
      "  BM25 MAP:  0.3274\n",
      "  BM25 NDCG: 0.4714\n",
      "  BERT MRR:  0.7962\n",
      "  BERT MAP:  0.4015\n",
      "  BERT NDCG: 0.5350\n",
      "p-value MRR: 0.0727\n",
      "p-value MAP: 0.0027\n",
      "p-value NDCG: 0.0121\n",
      "Time:  38.99887011243333  min\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 3e-5\n",
    "EPOCHS = 1\n",
    "train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "he1qC8Hj4rlT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================\n",
      "#               Hyper-Parameters\n",
      "Re-ranker\n",
      "bert-base-uncased\n",
      "3e-05\n",
      "128\n",
      "32\n",
      "2\n",
      "# ========================================\n",
      "#               Experiment-Settings\n",
      "CONVERSION TABLE:\n",
      "[[0, 1, 2, 3, 4], [-1, 1, 0.7, 0.53, 0.4]]\n",
      "# ========================================\n",
      "GPU Type: Tesla T4\n",
      "======== Fold 1 / 5 ========\n",
      "MRR:  0.7837\n",
      "MAP:  0.3493\n",
      "NDCG: 0.5011\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0252\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0147\n",
      "Testing...\n",
      "  Test MRR:  0.8732\n",
      "  Test MAP:  0.4336\n",
      "  Test NDCG: 0.5821\n",
      "45\n",
      "======== Fold 2 / 5 ========\n",
      "MRR:  0.6596\n",
      "MAP:  0.3036\n",
      "NDCG: 0.4546\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0287\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0148\n",
      "Testing...\n",
      "  Test MRR:  0.6991\n",
      "  Test MAP:  0.3555\n",
      "  Test NDCG: 0.5051\n",
      "90\n",
      "======== Fold 3 / 5 ========\n",
      "MRR:  0.7611\n",
      "MAP:  0.3341\n",
      "NDCG: 0.4826\n",
      "135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0221\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0138\n",
      "Testing...\n",
      "  Test MRR:  0.8631\n",
      "  Test MAP:  0.4480\n",
      "  Test NDCG: 0.5818\n",
      "135\n",
      "======== Fold 4 / 5 ========\n",
      "MRR:  0.6859\n",
      "MAP:  0.3317\n",
      "NDCG: 0.4408\n",
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0231\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0142\n",
      "Testing...\n",
      "  Test MRR:  0.7644\n",
      "  Test MAP:  0.3987\n",
      "  Test NDCG: 0.5014\n",
      "180\n",
      "======== Fold 5 / 5 ========\n",
      "MRR:  0.7796\n",
      "MAP:  0.3182\n",
      "NDCG: 0.4780\n",
      "225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0209\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0138\n",
      "Testing...\n",
      "  Test MRR:  0.8126\n",
      "  Test MAP:  0.4384\n",
      "  Test NDCG: 0.5805\n",
      "225\n",
      "  BM25 MRR:  0.7340\n",
      "  BM25 MAP:  0.3274\n",
      "  BM25 NDCG: 0.4714\n",
      "  BERT MRR:  0.8025\n",
      "  BERT MAP:  0.4148\n",
      "  BERT NDCG: 0.5502\n",
      "p-value MRR: 0.0469\n",
      "p-value MAP: 0.0005\n",
      "p-value NDCG: 0.0020\n",
      "Time:  82.16132233031666  min\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LEARNING_RATE = 3e-5\n",
    "EPOCHS = 2\n",
    "train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNiOcKVnK+YgqU/3IFi6iA9",
   "collapsed_sections": [],
   "mount_file_id": "1F7kxzcvWU4NOK0Qmwd8nAgrrLAxvRdrK",
   "name": "bert-cranfield-reranker-regresssion.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "common-cu110.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13cede8e4e26443e99deb0599e16cad7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5dc9687148f94e91992a3ddb1659dd25",
      "placeholder": "​",
      "style": "IPY_MODEL_3a2c1607b49741c3a9e027accb8125b7",
      "value": " 420M/420M [00:15&lt;00:00, 27.6MB/s]"
     }
    },
    "3a2c1607b49741c3a9e027accb8125b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ebab73c8f9a487fb5db5584b914540a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c3c7b15c4db4b55b5b2377bd513d327",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_daecc23330a34a65bf1479984b20f583",
      "value": 440473133
     }
    },
    "4193ea95c41740b3a9df08564e8619e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c3c7b15c4db4b55b5b2377bd513d327": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5dc9687148f94e91992a3ddb1659dd25": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d2931bd2c3a48ffb08d62185b54a05b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d2804829b444947a1f1186c8898d6d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0709620b1b449c995b4f0e565356a51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e70ff01f690542d6a36b5b995b6a8d5a",
       "IPY_MODEL_3ebab73c8f9a487fb5db5584b914540a",
       "IPY_MODEL_13cede8e4e26443e99deb0599e16cad7"
      ],
      "layout": "IPY_MODEL_8d2804829b444947a1f1186c8898d6d2"
     }
    },
    "daecc23330a34a65bf1479984b20f583": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e70ff01f690542d6a36b5b995b6a8d5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d2931bd2c3a48ffb08d62185b54a05b",
      "placeholder": "​",
      "style": "IPY_MODEL_4193ea95c41740b3a9df08564e8619e2",
      "value": "Downloading: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
