{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-t_ZMd4LeXgL"
   },
   "source": [
    "# BERT Meets Cranfield - MultiClass/Multilabel Training Approach\n",
    "\n",
    "**updates**\n",
    " - added custom single losses\n",
    " - added custom multi losses (enabeling multitask)\n",
    "\n",
    "The dataset has been labeled with multiple labels indicating how relevant a certain document is. The Cranfield description explains it as follows:\n",
    "1.  References which are a complete answer to the question.\n",
    "\n",
    "2.  References of a high degree of relevance, the lack of which either would have made the research impracticable or would have resulted in a considerable amount of extra work.\n",
    "\n",
    "3.  References which were useful, either as general background to the work or as suggesting methods of tackling certain aspects of the work.\n",
    "\n",
    "4.  References of minimum interest, for example, those that have been included from an historical viewpoint.\n",
    "\n",
    "5.  References of no interest. \n",
    "\n",
    "The following notebooks implements a functions that research wether and what method could make beneficial use of this relevance labeling.  \n",
    "\n",
    "*NOTE: Not all changes are in the notebook, a number of changes can be found in the `utils.py`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1632909785876,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "Fss4AAPjsdc2",
    "outputId": "d0961e30-b5ce-42e4-f5bb-912cc9a5e4bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/BERT-BM25-Thesis-Project/bert-meets-cranfield-multilabel/Code\n"
     ]
    }
   ],
   "source": [
    "# %cd /content/drive/MyDrive/COMPUTING SCIENCE/THESIS_PROJECT/BERT-BM25-Thesis-Project/bert-meets-cranfield-multilabel/Code\n",
    "%cd /home/jupyter/BERT-BM25-Thesis-Project/bert-meets-cranfield-multilabel/Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1632909786171,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "M4xg0ZyG2bqH",
    "outputId": "31308f1a-c7f8-4064-e412-e9fa87acac2e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3423,
     "status": "ok",
     "timestamp": 1632909789577,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "vRlcr2u0fL5q",
    "outputId": "4892c544-4277-4fa0-d85b-82168ae70aae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.19.1 in /opt/conda/lib/python3.7/site-packages (from -r ../requirements.txt (line 1)) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.5.2 in /opt/conda/lib/python3.7/site-packages (from -r ../requirements.txt (line 2)) (1.7.1)\n",
      "Requirement already satisfied: rank_bm25>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from -r ../requirements.txt (line 3)) (0.2.1)\n",
      "Requirement already satisfied: transformers>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from -r ../requirements.txt (line 4)) (4.10.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from -r ../requirements.txt (line 5)) (1.9.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (3.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (5.4.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (0.10.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (4.8.1)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (0.0.45)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (2021.8.28)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (4.62.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (2.25.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.12 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (0.0.17)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers>=3.1.0->-r ../requirements.txt (line 4)) (21.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->-r ../requirements.txt (line 5)) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers>=3.1.0->-r ../requirements.txt (line 4)) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers>=3.1.0->-r ../requirements.txt (line 4)) (3.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.1.0->-r ../requirements.txt (line 4)) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.1.0->-r ../requirements.txt (line 4)) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.1.0->-r ../requirements.txt (line 4)) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.1.0->-r ../requirements.txt (line 4)) (2021.5.30)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=3.1.0->-r ../requirements.txt (line 4)) (8.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=3.1.0->-r ../requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=3.1.0->-r ../requirements.txt (line 4)) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVLKhUhufNbM"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3465,
     "status": "ok",
     "timestamp": 1632909793023,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "mOM6kY43fQ7U"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import data_utils\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import importlib\n",
    "# from transformers import BertForSequenceClassification, BertTokenizer, BertForMaskedLM, BertForNextSentencePrediction\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRuHYyUifVsk"
   },
   "source": [
    "### Import Refresh\n",
    "When a supporting py-file (such as utils.py) is changed, this code will have the lib reloaded while not reloading the entire notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1632909793035,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "wDay4M8afTKL",
    "outputId": "c7ecbc25-a4e8-41b9-f010-3e37f80ecc1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_utils' from '/home/jupyter/BERT-BM25-Thesis-Project/bert-meets-cranfield-multilabel/Code/data_utils.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call after making any changes in utils.py\n",
    "importlib.reload(utils) \n",
    "importlib.reload(data_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mznu8BEmf2ds"
   },
   "source": [
    "## Set hyper-paramters and test settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1632909793035,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "ai5fO45Wf1qT"
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "#               Hyper-Parameters\n",
    "# ========================================\n",
    "SEED = 76\n",
    "MODE = 'Re-ranker'\n",
    "MODEL_TYPE = 'bert-base-uncased'\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "TOP_BM25 = 100\n",
    "MAP_CUT = 100\n",
    "NDCG_CUT = 20\n",
    "if MODE == 'Full-ranker':\n",
    "    TEST_BATCH_SIZE = 1400\n",
    "else:\n",
    "    TEST_BATCH_SIZE = 100\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "utils.initialize_random_generators(SEED)\n",
    "\n",
    "BM25_ENRICH = 'default' # or 'add' or 'swap' (default=no enrichment of BM25 results)\n",
    "MULTI_LABEL = True\n",
    "PER_LABEL_TESTING = False # if True, arg_max must be False\n",
    "'''\n",
    "  PER_LABEL_TESTING calculates the performance per predicted label. Contratry to\n",
    "  the binary case, each relevance levels 0-5 has a prediction. Method can be\n",
    "  seen as an alternative for the arg-max method. This flag is later implemented,\n",
    "  therefore it will mess up the adminstration for the calculation of final NDCG.\n",
    "  To circumvent this, the NDCG found for each fold, for each label has to be averaged.\n",
    "'''\n",
    "ARG_MAX_SORTING = True # if True, PER_LABEL_TESTING must be False\n",
    "CUSTOM_MODEL = 'multitask-BCEwLL-8020' # default is None, utils.py explains\n",
    "\n",
    "LOAD_CUSTOM_TRAINED_MODEL = False #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1632909793036,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "Ll05QrP3Qac7"
   },
   "outputs": [],
   "source": [
    "models_dir = \"/home/jupyter/BERT-BM25-Thesis-Project/Models/\" #@param {type:\"string\"}\n",
    "custom_model_name = \"BERT_Cranfield_MLM_model-128-16-5e-05-2.bin\" #@param {type:\"string\"}\n",
    "\n",
    "custom_model_path = models_dir + custom_model_name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpy2-0MM0T26"
   },
   "source": [
    "### Enriching function for BM25 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1632909793036,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "zstVSGg60T26"
   },
   "outputs": [],
   "source": [
    "def get_bm25_plus_other_rel(bm25_tn, labels, queries):\n",
    "      bm25_top_n_rel_padded = [0]*len(queries) # a bm25_top_n list padded with the remaining relevant documents\n",
    "      bm25_top_n_swap = [0]*len(queries) \n",
    "    \n",
    "      for qi in range(len(queries)):\n",
    "        # get the list of relelvant documents\n",
    "        lbi = np.where(labels[qi] == 1)\n",
    "        # note this numbering is only compatible with the labels list\n",
    "\n",
    "\n",
    "        # get the list of bm25_top_n\n",
    "        np_bm25_qi_docs = np.array(bm25_top_n[qi]) \n",
    "\n",
    "        # evaluate what relevant documents should be added\n",
    "        pad_rel = np.setdiff1d(lbi, np_bm25_qi_docs)\n",
    "        # if len(pad_rel) > 0:\n",
    "        pad_rel = tuple(pad_rel)\n",
    "        bm25_top_n_rel_padded[qi] = bm25_top_n[qi] + pad_rel\n",
    "        # create a list with least relevant items swapped for unfound relevant\n",
    "        for i in range(len(pad_rel)):\n",
    "          # CHECK\n",
    "          # are we to swap a relevant document?\n",
    "          current_doc = np_bm25_qi_docs[-(i+1)] \n",
    "          \n",
    "          if np.count_nonzero(current_doc == lbi) > 0:\n",
    "            print('Relevant doc overwritten!')\n",
    "          # CONTINUE  \n",
    "          np_bm25_qi_docs[-(i+1)] = pad_rel[i]\n",
    "          \n",
    "        bm25_top_n_swap[qi] = np_bm25_qi_docs\n",
    "      return bm25_top_n_rel_padded, bm25_top_n_swap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HLOroQocl_9"
   },
   "source": [
    "### Function for loading custom model\n",
    "Load in fact an encoder, that is trained with a specific specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1632909793037,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "8ULE9E5zdFen"
   },
   "outputs": [],
   "source": [
    "def load_specific_encoder(model_path):\n",
    "  '''\n",
    "    function to load saved encoder paramters\n",
    "\n",
    "    use this function to start every fold with a fresh model\n",
    "  '''\n",
    "  model = BertForSequenceClassification.from_pretrained(\n",
    "        MODEL_TYPE,\n",
    "        num_labels=2,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "    )\n",
    "  model.cuda\n",
    "  print('LOAD : ', model_path )\n",
    "\n",
    "  # =======================\n",
    "  # NOTE WHAT MODEL IS USED\n",
    "  model.load_state_dict(torch.load(model_path), strict=False)\n",
    "  # now you get a warning that extra training is required\n",
    "\n",
    "  if DO_FREEZING:\n",
    "    print('FREEZING: set requires_grad to False')\n",
    "    # freeze the encoder parameters (credits thomwolf of Huggingface)\n",
    "    # for param in model.bert.encoder.parameters():\n",
    "    #   param.requires_grad = False\n",
    "\n",
    "    # other method\n",
    "    model.bert.encoder.requires_grad_(False)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7og1VnHhgcsL"
   },
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1632909793310,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "3jAyQHGhgbFH"
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "def train_test():\n",
    "    print(\"# ========================================\")\n",
    "    print(\"#               Hyper-Parameters\")\n",
    "    print(MODE)\n",
    "    print(MODEL_TYPE)\n",
    "    print(LEARNING_RATE)\n",
    "    print(MAX_LENGTH)\n",
    "    print(BATCH_SIZE)\n",
    "    print(EPOCHS)\n",
    "    print(\"# ========================================\")\n",
    "    print(\"#               Experiment-Settings\")\n",
    "    print('BM25_ENRICHMENT:   ', BM25_ENRICH)\n",
    "    print('MULTI_LABEL:       ', MULTI_LABEL)\n",
    "    print('ARGMAX-SORTING:    ', ARG_MAX_SORTING)\n",
    "    print('PER_LABEL_TESTING: ', PER_LABEL_TESTING)\n",
    "    print('CUSTOM_MODEL:      ', CUSTOM_MODEL)\n",
    "\n",
    "\n",
    "    print(\"# ========================================\")\n",
    "    print(\"#               Other\")\n",
    "    print(torch.cuda.get_device_name())\n",
    "    print(\"# ========================================\")\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    device = utils.get_gpu_device()\n",
    "    if not os.path.exists('../Output_Folder'):\n",
    "        os.makedirs('../Output_Folder')\n",
    "\n",
    "    queries = data_utils.get_queries('../Data/cran/cran.qry')\n",
    "    corpus = data_utils.get_corpus('../Data/cran/cran.all.1400')\n",
    "    rel_fed = data_utils.get_judgments('../Data/cran/cranqrel')\n",
    "\n",
    "    labels = utils.get_binary_labels(rel_fed, multilabel=MULTI_LABEL)\n",
    "    tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "    tokenized_queries = [query.split(\" \") for query in queries]\n",
    "\n",
    "    bm25, bm25_top_n = utils.get_bm25_top_results(tokenized_corpus, tokenized_queries, TOP_BM25)\n",
    "\n",
    "    # no matter what BM25_ENRICH is, this line is needed to get `temp_feedback` for the test set\n",
    "    padded_all, attention_mask_all, token_type_ids_all, temp_feedback = utils.bert_tokenizer(MODE, bm25_top_n, corpus,\n",
    "                                                                                             labels, queries,\n",
    "                                                                                             MAX_LENGTH, MODEL_TYPE)\n",
    "    if BM25_ENRICH == 'swap':\n",
    "        bm25_top_n_ext, bm25_top_n_swap = get_bm25_plus_other_rel(bm25_top_n, labels, queries)\n",
    "        padded_all_swap, attention_mask_all_swap, token_type_ids_all_swap, temp_feedback_swap = utils.bert_tokenizer(MODE, bm25_top_n_swap, corpus,\n",
    "                                                                                                                     labels, queries,\n",
    "                                                                                                                     MAX_LENGTH, MODEL_TYPE)\n",
    "    elif BM25_ENRICH == 'add':\n",
    "        bm25_top_n_add, bm25_top_n_swap = get_bm25_plus_other_rel(bm25_top_n, labels, queries)\n",
    "        padded_all_add, attention_mask_all_add, token_type_ids_all_add, temp_feedback_add = utils.bert_tokenizer(MODE, bm25_top_n_add, corpus,\n",
    "                                                                                                                 labels, queries,\n",
    "                                                                                                                 MAX_LENGTH, MODEL_TYPE)\n",
    "\n",
    "    # ========================================\n",
    "    #               Folds\n",
    "    # ========================================\n",
    "    mrr_bm25_list, map_bm25_list, ndcg_bm25_list = [], [], []\n",
    "    mrr_bert_list, map_bert_list, ndcg_bert_list = [], [], []\n",
    "    mrr_bm25, map_bm25, ndcg_bm25 = 0, 0, 0\n",
    "    mrr_bert, map_bert, ndcg_bert = 0, 0, 0\n",
    "\n",
    "    for fold_number in range(1, 6):\n",
    "        print('======== Fold {:} / {:} ========'.format(fold_number, 5))\n",
    "        train_index, test_index = data_utils.load_fold(fold_number)\n",
    "\n",
    "        padded, attention_mask, token_type_ids = [], [], []\n",
    "        if MODE == 'Re-ranker':\n",
    "            # no matter BM25_ENRICH-mode, next line required for test set construction\n",
    "            padded, attention_mask, token_type_ids = padded_all, attention_mask_all, token_type_ids_all\n",
    "            if BM25_ENRICH == 'swap':\n",
    "                padded_swap, attention_mask_swap, token_type_ids_swap = padded_all_swap, attention_mask_all_swap, token_type_ids_all_swap\n",
    "            elif BM25_ENRICH == 'add':\n",
    "                padded_add, attention_mask_add, token_type_ids_add = padded_all_add, attention_mask_all_add, token_type_ids_all_add\n",
    "            \n",
    "        else:\n",
    "            temp_feedback = []\n",
    "            for query_num in range(0, len(bm25_top_n)):\n",
    "                if query_num in test_index:\n",
    "                    doc_nums = range(0, 1400)\n",
    "                else:\n",
    "                    doc_nums = bm25_top_n[query_num]\n",
    "                padded.append(list(itemgetter(*doc_nums)(padded_all[query_num])))\n",
    "                attention_mask.append(list(itemgetter(*doc_nums)(attention_mask_all[query_num])))\n",
    "                token_type_ids.append(list(itemgetter(*doc_nums)(token_type_ids_all[query_num])))\n",
    "                temp_feedback.append(list(itemgetter(*doc_nums)(labels[query_num])))\n",
    "\n",
    "        # Enricht the training set (or keep default)\n",
    "        if BM25_ENRICH == 'default':\n",
    "            train_dataset = data_utils.get_tensor_dataset(train_index, padded, attention_mask, token_type_ids,\n",
    "                                                          temp_feedback)\n",
    "        elif BM25_ENRICH == 'swap':\n",
    "            train_dataset = data_utils.get_tensor_dataset(train_index, padded_swap, attention_mask_swap, token_type_ids_swap,\n",
    "                                                    temp_feedback_swap)\n",
    "        elif BM25_ENRICH == 'add':\n",
    "            train_dataset = data_utils.get_tensor_dataset(train_index, padded_add, attention_mask_add, token_type_ids_add,\n",
    "                                                    temp_feedback_add)\n",
    "\n",
    "        test_dataset = data_utils.get_tensor_dataset(test_index, padded, attention_mask, token_type_ids, temp_feedback)\n",
    "\n",
    "        mrr_bm25, map_bm25, ndcg_bm25, mrr_bm25_list, map_bm25_list, ndcg_bm25_list = utils.get_bm25_results(\n",
    "            mrr_bm25_list, map_bm25_list, ndcg_bm25_list, test_index, tokenized_queries, bm25, mrr_bm25, map_bm25,\n",
    "            ndcg_bm25, rel_fed, fold_number, MAP_CUT, NDCG_CUT)\n",
    "\n",
    "          \n",
    "        # Option to load a custom trained model (used in transfer learning)\n",
    "        if LOAD_CUSTOM_TRAINED_MODEL:\n",
    "          model = load_specific_encoder(custom_model_path)\n",
    "        else:\n",
    "          model = None\n",
    "          # with None the model_preparation loads the 'MODEL_TYPE' model\n",
    "        if MULTI_LABEL:\n",
    "          num_labels = 5\n",
    "        else:\n",
    "          num_labels = 2\n",
    "        train_dataloader, test_dataloader, model, optimizer, scheduler = utils.model_preparation(MODEL_TYPE, train_dataset,\n",
    "                                                                                                 test_dataset,\n",
    "                                                                                                 BATCH_SIZE, TEST_BATCH_SIZE,\n",
    "                                                                                                 LEARNING_RATE, EPOCHS, model=model,\n",
    "                                                                                                 num_labels=num_labels,\n",
    "                                                                                                 custom_model=CUSTOM_MODEL)\n",
    "\n",
    "\n",
    "        # ========================================\n",
    "        #               Training Loop\n",
    "        # ========================================\n",
    "        epochs_train_loss, epochs_val_loss = [], []\n",
    "        for epoch_i in range(0, EPOCHS):\n",
    "            # ========================================\n",
    "            #               Training\n",
    "            # ========================================\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n",
    "            print('Training...')\n",
    "            model, optimizer, scheduler = utils.training(model, train_dataloader, device, optimizer, scheduler)\n",
    "        # ========================================\n",
    "        #               Testing\n",
    "        # ========================================\n",
    "        print('Testing...')\n",
    "        mrr_bert, map_bert, ndcg_bert, mrr_bert_list, map_bert_list, ndcg_bert_list = utils.testing(MODE, model,\n",
    "                                                                                                    test_dataloader,\n",
    "                                                                                                    device, test_index,\n",
    "                                                                                                    bm25_top_n,\n",
    "                                                                                                    mrr_bert_list,\n",
    "                                                                                                    map_bert_list,\n",
    "                                                                                                    ndcg_bert_list,\n",
    "                                                                                                    mrr_bert, map_bert,\n",
    "                                                                                                    ndcg_bert, rel_fed,\n",
    "                                                                                                    fold_number,\n",
    "                                                                                                    MAP_CUT, NDCG_CUT,\n",
    "                                                                                                    multilabel=MULTI_LABEL,\n",
    "                                                                                                    argmax_sorting=ARG_MAX_SORTING,\n",
    "                                                                                                    per_label_testing=PER_LABEL_TESTING)\n",
    "    print(\"  BM25 MRR:  \" + \"{:.4f}\".format(mrr_bm25 / 5))\n",
    "    print(\"  BM25 MAP:  \" + \"{:.4f}\".format(map_bm25 / 5))\n",
    "    print(\"  BM25 NDCG: \" + \"{:.4f}\".format(ndcg_bm25 / 5))\n",
    "\n",
    "    print(\"  BERT MRR:  \" + \"{:.4f}\".format(mrr_bert / 5))\n",
    "    print(\"  BERT MAP:  \" + \"{:.4f}\".format(map_bert / 5))\n",
    "    print(\"  BERT NDCG: \" + \"{:.4f}\".format(ndcg_bert / 5))\n",
    "\n",
    "    utils.t_test(mrr_bm25_list, mrr_bert_list, 'MRR')\n",
    "    utils.t_test(map_bm25_list, map_bert_list, 'MAP')\n",
    "    utils.t_test(ndcg_bm25_list, ndcg_bert_list, 'NDCG')\n",
    "    \n",
    "    stop = timeit.default_timer()\n",
    "    wall_time = (stop - start) / 60 \n",
    "\n",
    "    print('Time: ', wall_time, ' min') \n",
    "\n",
    "    # utils.results_to_csv('./mrr_bm25_list.csv', mrr_bm25_list)\n",
    "    # utils.results_to_csv('./mrr_bert_list.csv', mrr_bert_list)\n",
    "    # utils.results_to_csv('./map_bm25_list.csv', map_bm25_list)\n",
    "    # utils.results_to_csv('./map_bert_list.csv', map_bert_list)\n",
    "    # utils.results_to_csv('./ndcg_bm25_list.csv', ndcg_bm25_list)\n",
    "    # utils.results_to_csv('./ndcg_bert_list.csv', ndcg_bert_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4001775,
     "status": "ok",
     "timestamp": 1632913795071,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "b-DEy5MapwMc",
    "outputId": "978bc19a-754c-43f0-c430-51468fbbdbb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================\n",
      "#               Hyper-Parameters\n",
      "Re-ranker\n",
      "bert-base-uncased\n",
      "2e-05\n",
      "128\n",
      "32\n",
      "2\n",
      "# ========================================\n",
      "#               Experiment-Settings\n",
      "BM25_ENRICHMENT:    default\n",
      "MULTI_LABEL:        True\n",
      "ARGMAX-SORTING:     True\n",
      "PER_LABEL_TESTING:  False\n",
      "CUSTOM_MODEL:       multitask-BCEwLL-5050\n",
      "# ========================================\n",
      "#               Other\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla T4\n",
      "# ========================================\n",
      "GPU Type: Tesla T4\n",
      "======== Fold 1 / 5 ========\n",
      "MRR:  0.7837\n",
      "MAP:  0.3493\n",
      "NDCG: 0.5011\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'classifier.weight', 'rel_or_not_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1472\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0943\n",
      "Testing...\n",
      "  Test MRR:  0.8413\n",
      "  Test MAP:  0.4197\n",
      "  Test NDCG: 0.5631\n",
      "45\n",
      "======== Fold 2 / 5 ========\n",
      "MRR:  0.6596\n",
      "MAP:  0.3036\n",
      "NDCG: 0.4546\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'classifier.weight', 'rel_or_not_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1498\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0971\n",
      "Testing...\n",
      "  Test MRR:  0.7118\n",
      "  Test MAP:  0.3535\n",
      "  Test NDCG: 0.4979\n",
      "90\n",
      "======== Fold 3 / 5 ========\n",
      "MRR:  0.7611\n",
      "MAP:  0.3341\n",
      "NDCG: 0.4826\n",
      "135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'classifier.weight', 'rel_or_not_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1437\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0924\n",
      "Testing...\n",
      "  Test MRR:  0.8813\n",
      "  Test MAP:  0.4443\n",
      "  Test NDCG: 0.5873\n",
      "135\n",
      "======== Fold 4 / 5 ========\n",
      "MRR:  0.6859\n",
      "MAP:  0.3317\n",
      "NDCG: 0.4408\n",
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'classifier.weight', 'rel_or_not_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1402\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0904\n",
      "Testing...\n",
      "  Test MRR:  0.7249\n",
      "  Test MAP:  0.3873\n",
      "  Test NDCG: 0.4937\n",
      "180\n",
      "======== Fold 5 / 5 ========\n",
      "MRR:  0.7796\n",
      "MAP:  0.3182\n",
      "NDCG: 0.4780\n",
      "225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'classifier.weight', 'rel_or_not_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1434\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0876\n",
      "Testing...\n",
      "  Test MRR:  0.8493\n",
      "  Test MAP:  0.4188\n",
      "  Test NDCG: 0.5682\n",
      "225\n",
      "  BM25 MRR:  0.7340\n",
      "  BM25 MAP:  0.3274\n",
      "  BM25 NDCG: 0.4714\n",
      "  BERT MRR:  0.8017\n",
      "  BERT MAP:  0.4047\n",
      "  BERT NDCG: 0.5420\n",
      "p-value MRR: 0.0491\n",
      "p-value MAP: 0.0016\n",
      "p-value NDCG: 0.0050\n",
      "Time:  73.85459783443333  min\n"
     ]
    }
   ],
   "source": [
    "train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1632913795071,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "lIIOcbG365PU",
    "outputId": "ce1c5968-a94a-44a9-a0e8-d14ac1f03ee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================\n",
      "#               Hyper-Parameters\n",
      "Re-ranker\n",
      "bert-base-uncased\n",
      "3e-05\n",
      "128\n",
      "32\n",
      "1\n",
      "# ========================================\n",
      "#               Experiment-Settings\n",
      "BM25_ENRICHMENT:    default\n",
      "MULTI_LABEL:        True\n",
      "ARGMAX-SORTING:     True\n",
      "PER_LABEL_TESTING:  False\n",
      "CUSTOM_MODEL:       multitask-BCEwLL-5050\n",
      "# ========================================\n",
      "#               Other\n",
      "Tesla T4\n",
      "# ========================================\n",
      "GPU Type: Tesla T4\n",
      "======== Fold 1 / 5 ========\n",
      "MRR:  0.7837\n",
      "MAP:  0.3493\n",
      "NDCG: 0.5011\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'classifier.weight', 'rel_or_not_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1407\n",
      "Testing...\n",
      "  Test MRR:  0.8442\n",
      "  Test MAP:  0.4104\n",
      "  Test NDCG: 0.5512\n",
      "45\n",
      "======== Fold 2 / 5 ========\n",
      "MRR:  0.6596\n",
      "MAP:  0.3036\n",
      "NDCG: 0.4546\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'classifier.weight', 'rel_or_not_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1378\n",
      "Testing...\n",
      "  Test MRR:  0.7696\n",
      "  Test MAP:  0.3608\n",
      "  Test NDCG: 0.5179\n",
      "90\n",
      "======== Fold 3 / 5 ========\n",
      "MRR:  0.7611\n",
      "MAP:  0.3341\n",
      "NDCG: 0.4826\n",
      "135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'classifier.weight', 'rel_or_not_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1429\n",
      "Testing...\n",
      "  Test MRR:  0.7972\n",
      "  Test MAP:  0.4229\n",
      "  Test NDCG: 0.5632\n",
      "135\n",
      "======== Fold 4 / 5 ========\n",
      "MRR:  0.6859\n",
      "MAP:  0.3317\n",
      "NDCG: 0.4408\n",
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'classifier.weight', 'rel_or_not_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1387\n",
      "Testing...\n",
      "  Test MRR:  0.7082\n",
      "  Test MAP:  0.3776\n",
      "  Test NDCG: 0.4622\n",
      "180\n",
      "======== Fold 5 / 5 ========\n",
      "MRR:  0.7796\n",
      "MAP:  0.3182\n",
      "NDCG: 0.4780\n",
      "225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'classifier.weight', 'rel_or_not_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1379\n",
      "Testing...\n",
      "  Test MRR:  0.7797\n",
      "  Test MAP:  0.3901\n",
      "  Test NDCG: 0.5365\n",
      "225\n",
      "  BM25 MRR:  0.7340\n",
      "  BM25 MAP:  0.3274\n",
      "  BM25 NDCG: 0.4714\n",
      "  BERT MRR:  0.7798\n",
      "  BERT MAP:  0.3923\n",
      "  BERT NDCG: 0.5262\n",
      "p-value MRR: 0.1882\n",
      "p-value MAP: 0.0077\n",
      "p-value NDCG: 0.0286\n",
      "Time:  39.39729729924999  min\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 3e-5\n",
    "EPOCHS = 1\n",
    "train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1632913795073,
     "user": {
      "displayName": "Maurice Verbrugge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqmdBO0giQPluidHjemi6aWvts_C_AZ5rqt1Y4yA=s64",
      "userId": "18106956806494538064"
     },
     "user_tz": -120
    },
    "id": "fMV3ejst67Vt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================\n",
      "#               Hyper-Parameters\n",
      "Re-ranker\n",
      "bert-base-uncased\n",
      "3e-05\n",
      "128\n",
      "32\n",
      "2\n",
      "# ========================================\n",
      "#               Experiment-Settings\n",
      "BM25_ENRICHMENT:    default\n",
      "MULTI_LABEL:        True\n",
      "ARGMAX-SORTING:     True\n",
      "PER_LABEL_TESTING:  False\n",
      "CUSTOM_MODEL:       multitask-BCEwLL-5050\n",
      "# ========================================\n",
      "#               Other\n",
      "Tesla T4\n",
      "# ========================================\n",
      "GPU Type: Tesla T4\n",
      "======== Fold 1 / 5 ========\n",
      "MRR:  0.7837\n",
      "MAP:  0.3493\n",
      "NDCG: 0.5011\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'classifier.weight', 'rel_or_not_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1515\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1006\n",
      "Testing...\n",
      "  Test MRR:  0.7755\n",
      "  Test MAP:  0.3939\n",
      "  Test NDCG: 0.5365\n",
      "45\n",
      "======== Fold 2 / 5 ========\n",
      "MRR:  0.6596\n",
      "MAP:  0.3036\n",
      "NDCG: 0.4546\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'classifier.weight', 'rel_or_not_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1475\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1472\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0956\n",
      "Testing...\n",
      "  Test MRR:  0.8561\n",
      "  Test MAP:  0.4374\n",
      "  Test NDCG: 0.5791\n",
      "135\n",
      "======== Fold 4 / 5 ========\n",
      "MRR:  0.6859\n",
      "MAP:  0.3317\n",
      "NDCG: 0.4408\n",
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'classifier.weight', 'rel_or_not_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1397\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0905\n",
      "Testing...\n",
      "  Test MRR:  0.7520\n",
      "  Test MAP:  0.4082\n",
      "  Test NDCG: 0.5070\n",
      "180\n",
      "======== Fold 5 / 5 ========\n",
      "MRR:  0.7796\n",
      "MAP:  0.3182\n",
      "NDCG: 0.4780\n",
      "225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'classifier.weight', 'rel_or_not_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1399\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0906\n",
      "Testing...\n",
      "  Test MRR:  0.8316\n",
      "  Test MAP:  0.4301\n",
      "  Test NDCG: 0.5823\n",
      "225\n",
      "  BM25 MRR:  0.7340\n",
      "  BM25 MAP:  0.3274\n",
      "  BM25 NDCG: 0.4714\n",
      "  BERT MRR:  0.7900\n",
      "  BERT MAP:  0.4052\n",
      "  BERT NDCG: 0.5430\n",
      "p-value MRR: 0.1044\n",
      "p-value MAP: 0.0015\n",
      "p-value NDCG: 0.0042\n",
      "Time:  73.56272463035  min\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 3e-5\n",
    "EPOCHS = 2\n",
    "train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================\n",
      "#               Hyper-Parameters\n",
      "Re-ranker\n",
      "bert-base-uncased\n",
      "2e-05\n",
      "128\n",
      "32\n",
      "1\n",
      "# ========================================\n",
      "#               Experiment-Settings\n",
      "BM25_ENRICHMENT:    default\n",
      "MULTI_LABEL:        True\n",
      "ARGMAX-SORTING:     True\n",
      "PER_LABEL_TESTING:  False\n",
      "CUSTOM_MODEL:       multitask-BCEwLL-8020\n",
      "# ========================================\n",
      "#               Other\n",
      "Tesla T4\n",
      "# ========================================\n",
      "GPU Type: Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Fold 1 / 5 ========\n",
      "MRR:  0.7837\n",
      "MAP:  0.3493\n",
      "NDCG: 0.5011\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'rel_or_not_classifier.weight', 'rel_or_not_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1658\n",
      "Testing...\n",
      "  Test MRR:  0.8391\n",
      "  Test MAP:  0.3952\n",
      "  Test NDCG: 0.5356\n",
      "45\n",
      "======== Fold 2 / 5 ========\n",
      "MRR:  0.6596\n",
      "MAP:  0.3036\n",
      "NDCG: 0.4546\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'rel_or_not_classifier.weight', 'rel_or_not_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1652\n",
      "Testing...\n",
      "  Test MRR:  0.7038\n",
      "  Test MAP:  0.3175\n",
      "  Test NDCG: 0.4604\n",
      "90\n",
      "======== Fold 3 / 5 ========\n",
      "MRR:  0.7611\n",
      "MAP:  0.3341\n",
      "NDCG: 0.4826\n",
      "135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'rel_or_not_classifier.weight', 'rel_or_not_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1688\n",
      "Testing...\n",
      "  Test MRR:  0.8698\n",
      "  Test MAP:  0.4400\n",
      "  Test NDCG: 0.5775\n",
      "135\n",
      "======== Fold 4 / 5 ========\n",
      "MRR:  0.6859\n",
      "MAP:  0.3317\n",
      "NDCG: 0.4408\n",
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'rel_or_not_classifier.weight', 'rel_or_not_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1599\n",
      "Testing...\n",
      "  Test MRR:  0.7151\n",
      "  Test MAP:  0.3724\n",
      "  Test NDCG: 0.4657\n",
      "180\n",
      "======== Fold 5 / 5 ========\n",
      "MRR:  0.7796\n",
      "MAP:  0.3182\n",
      "NDCG: 0.4780\n",
      "225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'rel_or_not_classifier.weight', 'rel_or_not_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1638\n",
      "Testing...\n",
      "  Test MRR:  0.8278\n",
      "  Test MAP:  0.4230\n",
      "  Test NDCG: 0.5726\n",
      "225\n",
      "  BM25 MRR:  0.7340\n",
      "  BM25 MAP:  0.3274\n",
      "  BM25 NDCG: 0.4714\n",
      "  BERT MRR:  0.7911\n",
      "  BERT MAP:  0.3896\n",
      "  BERT NDCG: 0.5224\n",
      "p-value MRR: 0.1009\n",
      "p-value MAP: 0.0107\n",
      "p-value NDCG: 0.0418\n",
      "Time:  39.41220719816668  min\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 1\n",
    "train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================\n",
      "#               Hyper-Parameters\n",
      "Re-ranker\n",
      "bert-base-uncased\n",
      "2e-05\n",
      "128\n",
      "32\n",
      "2\n",
      "# ========================================\n",
      "#               Experiment-Settings\n",
      "BM25_ENRICHMENT:    default\n",
      "MULTI_LABEL:        True\n",
      "ARGMAX-SORTING:     True\n",
      "PER_LABEL_TESTING:  False\n",
      "CUSTOM_MODEL:       multitask-BCEwLL-8020\n",
      "# ========================================\n",
      "#               Other\n",
      "Tesla T4\n",
      "# ========================================\n",
      "GPU Type: Tesla T4\n",
      "======== Fold 1 / 5 ========\n",
      "MRR:  0.7837\n",
      "MAP:  0.3493\n",
      "NDCG: 0.5011\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'rel_or_not_classifier.weight', 'rel_or_not_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1605\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1062\n",
      "Testing...\n",
      "  Test MRR:  0.8263\n",
      "  Test MAP:  0.4098\n",
      "  Test NDCG: 0.5516\n",
      "45\n",
      "======== Fold 2 / 5 ========\n",
      "MRR:  0.6596\n",
      "MAP:  0.3036\n",
      "NDCG: 0.4546\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'rel_or_not_classifier.weight', 'rel_or_not_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1673\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1076\n",
      "Testing...\n",
      "  Test MRR:  0.7346\n",
      "  Test MAP:  0.3651\n",
      "  Test NDCG: 0.5016\n",
      "90\n",
      "======== Fold 3 / 5 ========\n",
      "MRR:  0.7611\n",
      "MAP:  0.3341\n",
      "NDCG: 0.4826\n",
      "135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'rel_or_not_classifier.weight', 'rel_or_not_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1675\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1092\n",
      "Testing...\n",
      "  Test MRR:  0.8739\n",
      "  Test MAP:  0.4311\n",
      "  Test NDCG: 0.5609\n",
      "135\n",
      "======== Fold 4 / 5 ========\n",
      "MRR:  0.6859\n",
      "MAP:  0.3317\n",
      "NDCG: 0.4408\n",
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'rel_or_not_classifier.weight', 'rel_or_not_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1680\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1083\n",
      "Testing...\n",
      "  Test MRR:  0.7645\n",
      "  Test MAP:  0.3963\n",
      "  Test NDCG: 0.5031\n",
      "180\n",
      "======== Fold 5 / 5 ========\n",
      "MRR:  0.7796\n",
      "MAP:  0.3182\n",
      "NDCG: 0.4780\n",
      "225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'rel_or_not_classifier.weight', 'rel_or_not_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1637\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1045\n",
      "Testing...\n",
      "  Test MRR:  0.8175\n",
      "  Test MAP:  0.4227\n",
      "  Test NDCG: 0.5676\n",
      "225\n",
      "  BM25 MRR:  0.7340\n",
      "  BM25 MAP:  0.3274\n",
      "  BM25 NDCG: 0.4714\n",
      "  BERT MRR:  0.8034\n",
      "  BERT MAP:  0.4050\n",
      "  BERT NDCG: 0.5370\n",
      "p-value MRR: 0.0439\n",
      "p-value MAP: 0.0015\n",
      "p-value NDCG: 0.0091\n",
      "Time:  73.57310875458337  min\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 2\n",
    "train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================\n",
      "#               Hyper-Parameters\n",
      "Re-ranker\n",
      "bert-base-uncased\n",
      "3e-05\n",
      "128\n",
      "32\n",
      "1\n",
      "# ========================================\n",
      "#               Experiment-Settings\n",
      "BM25_ENRICHMENT:    default\n",
      "MULTI_LABEL:        True\n",
      "ARGMAX-SORTING:     True\n",
      "PER_LABEL_TESTING:  False\n",
      "CUSTOM_MODEL:       multitask-BCEwLL-8020\n",
      "# ========================================\n",
      "#               Other\n",
      "Tesla T4\n",
      "# ========================================\n",
      "GPU Type: Tesla T4\n",
      "======== Fold 1 / 5 ========\n",
      "MRR:  0.7837\n",
      "MAP:  0.3493\n",
      "NDCG: 0.5011\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'rel_or_not_classifier.weight', 'rel_or_not_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1604\n",
      "Testing...\n",
      "  Test MRR:  0.8151\n",
      "  Test MAP:  0.4139\n",
      "  Test NDCG: 0.5525\n",
      "45\n",
      "======== Fold 2 / 5 ========\n",
      "MRR:  0.6596\n",
      "MAP:  0.3036\n",
      "NDCG: 0.4546\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'rel_or_not_classifier.weight', 'rel_or_not_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1889\n",
      "Testing...\n",
      "  Test MRR:  0.8502\n",
      "  Test MAP:  0.4076\n",
      "  Test NDCG: 0.5488\n",
      "135\n",
      "======== Fold 4 / 5 ========\n",
      "MRR:  0.6859\n",
      "MAP:  0.3317\n",
      "NDCG: 0.4408\n",
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'rel_or_not_classifier.weight', 'rel_or_not_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1497\n",
      "Testing...\n",
      "  Test MRR:  0.7351\n",
      "  Test MAP:  0.4094\n",
      "  Test NDCG: 0.5080\n",
      "180\n",
      "======== Fold 5 / 5 ========\n",
      "MRR:  0.7796\n",
      "MAP:  0.3182\n",
      "NDCG: 0.4780\n",
      "225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'rel_or_not_classifier.weight', 'rel_or_not_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1585\n",
      "Testing...\n",
      "  Test MRR:  0.8532\n",
      "  Test MAP:  0.4202\n",
      "  Test NDCG: 0.5736\n",
      "225\n",
      "  BM25 MRR:  0.7340\n",
      "  BM25 MAP:  0.3274\n",
      "  BM25 NDCG: 0.4714\n",
      "  BERT MRR:  0.7930\n",
      "  BERT MAP:  0.4001\n",
      "  BERT NDCG: 0.5339\n",
      "p-value MRR: 0.0870\n",
      "p-value MAP: 0.0030\n",
      "p-value NDCG: 0.0123\n",
      "Time:  39.366000733116685  min\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 3e-5\n",
    "EPOCHS = 1\n",
    "train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================\n",
      "#               Hyper-Parameters\n",
      "Re-ranker\n",
      "bert-base-uncased\n",
      "3e-05\n",
      "128\n",
      "32\n",
      "2\n",
      "# ========================================\n",
      "#               Experiment-Settings\n",
      "BM25_ENRICHMENT:    default\n",
      "MULTI_LABEL:        True\n",
      "ARGMAX-SORTING:     True\n",
      "PER_LABEL_TESTING:  False\n",
      "CUSTOM_MODEL:       multitask-BCEwLL-8020\n",
      "# ========================================\n",
      "#               Other\n",
      "Tesla T4\n",
      "# ========================================\n",
      "GPU Type: Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Fold 1 / 5 ========\n",
      "MRR:  0.7837\n",
      "MAP:  0.3493\n",
      "NDCG: 0.5011\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'rel_or_not_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1686\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1109\n",
      "Testing...\n",
      "  Test MRR:  0.8419\n",
      "  Test MAP:  0.4166\n",
      "  Test NDCG: 0.5603\n",
      "45\n",
      "======== Fold 2 / 5 ========\n",
      "MRR:  0.6596\n",
      "MAP:  0.3036\n",
      "NDCG: 0.4546\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'rel_or_not_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1563\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1007\n",
      "Testing...\n",
      "  Test MRR:  0.7447\n",
      "  Test MAP:  0.3678\n",
      "  Test NDCG: 0.5168\n",
      "90\n",
      "======== Fold 3 / 5 ========\n",
      "MRR:  0.7611\n",
      "MAP:  0.3341\n",
      "NDCG: 0.4826\n",
      "135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'rel_or_not_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1576\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1065\n",
      "Testing...\n",
      "  Test MRR:  0.8321\n",
      "  Test MAP:  0.4361\n",
      "  Test NDCG: 0.5725\n",
      "135\n",
      "======== Fold 4 / 5 ========\n",
      "MRR:  0.6859\n",
      "MAP:  0.3317\n",
      "NDCG: 0.4408\n",
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'rel_or_not_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1499\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0967\n",
      "Testing...\n",
      "  Test MRR:  0.7579\n",
      "  Test MAP:  0.4042\n",
      "  Test NDCG: 0.5085\n",
      "180\n",
      "======== Fold 5 / 5 ========\n",
      "MRR:  0.7796\n",
      "MAP:  0.3182\n",
      "NDCG: 0.4780\n",
      "225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiTaskSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['rel_or_not_classifier.bias', 'rel_or_not_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.1527\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    563.\n",
      "  Batch   200  of    563.\n",
      "  Batch   300  of    563.\n",
      "  Batch   400  of    563.\n",
      "  Batch   500  of    563.\n",
      "  Average training loss: 0.0962\n",
      "Testing...\n",
      "  Test MRR:  0.8467\n",
      "  Test MAP:  0.4324\n",
      "  Test NDCG: 0.5766\n",
      "225\n",
      "  BM25 MRR:  0.7340\n",
      "  BM25 MAP:  0.3274\n",
      "  BM25 NDCG: 0.4714\n",
      "  BERT MRR:  0.8047\n",
      "  BERT MAP:  0.4114\n",
      "  BERT NDCG: 0.5469\n",
      "p-value MRR: 0.0377\n",
      "p-value MAP: 0.0007\n",
      "p-value NDCG: 0.0026\n",
      "Time:  72.3123130827  min\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 3e-5\n",
    "EPOCHS = 2\n",
    "train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "multi-label-training-multi-loss.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "common-cu110.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m79"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
